{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def pad_input_matrix(X, pad: int, padding_value: int = 0):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C) # m is omitted for now\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, pad_width=((0, 0), (pad, pad), (pad, pad)),\n",
    "                   mode='constant', constant_values=(padding_value, padding_value))\n",
    "    # X_pad = np.pad(X, ((0,0),(pad,pad),(pad,pad),(0,0)))\n",
    "\n",
    "    return X_pad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    # number of output channel = filter counts\n",
    "    def __init__(self, input_shape: (int, int, int), number_of_output_channel: int, filter_dimension: int,\n",
    "                 stride: int, padding: int):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.number_of_output_channel = number_of_output_channel\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "\n",
    "        self.output_shape = (number_of_output_channel, input_height - filter_dimension + 1, input_width\n",
    "                             - filter_dimension + 1)\n",
    "\n",
    "        self.filters_shape = (number_of_output_channel, input_depth, filter_dimension, filter_dimension)\n",
    "        self.filters = np.random.randn(*self.filters_shape)\n",
    "\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "\n",
    "    def convolution_single_step(input_slice, filter_mat):\n",
    "        \"\"\"\n",
    "        Apply one filter defined by parameters weight on a single slice (a_slice_prev) of the output activation\n",
    "        of the previous layer.\n",
    "\n",
    "        Arguments:\n",
    "        a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "        weight -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "        bias -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "        Returns:\n",
    "        Z -- a scalar value, the result of convolving the sliding window (weight, bias) on a slice x of the input data\n",
    "        \"\"\"\n",
    "\n",
    "        #(≈ 3 lines of code)\n",
    "        # Element-wise product between a_slice_prev and weight. Do not add the bias yet.\n",
    "        s = np.multiply(a_slice_prev, weight)\n",
    "        # Sum over all entries of the volume s.\n",
    "        Z = np.sum(s)\n",
    "        # Add bias to Z. Cast bias to a float() so that Z results in a scalar value.\n",
    "        bias = np.squeeze(bias)\n",
    "        Z = Z + bias\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "    def conv_forward(self, A_prev, W, b):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer,\n",
    "            numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "\n",
    "        Returns:\n",
    "        Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward() function\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape (≈1 line)\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "        # Retrieve dimensions from W's shape (≈1 line)\n",
    "        (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "        # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "        # stride = hparameters[\"stride\"]\n",
    "        # pad = hparameters[\"pad\"]\n",
    "\n",
    "        # Compute the dimensions of the CONV output volume using the formula given above.\n",
    "        # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
    "        n_H = int((n_H_prev + 2*self.padding - f)/self.stride) + 1\n",
    "        n_W = int((n_W_prev + 2*self.padding - f)/self.stride) + 1\n",
    "\n",
    "        # Initialize the output volume Z with zeros. (≈1 line)\n",
    "        Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "        # Create A_prev_pad by padding A_prev\n",
    "        A_prev_pad = zero_pad(A_prev, self.padding)\n",
    "\n",
    "        for i in range(m):               # loop over the batch of training examples\n",
    "            a_prev_pad = A_prev_pad[i]          # Select ith training example's padded activation\n",
    "            for h in range(n_H):           # loop over vertical axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                vert_start = self.stride * h\n",
    "                vert_end = vert_start  + f\n",
    "\n",
    "                for w in range(n_W):       # loop over horizontal axis of the output volume\n",
    "                    # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "                    horiz_start = self.stride * w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                        # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                        a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                        # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                        weights = W[:, :, :, c]\n",
    "                        biases  = b[:, :, :, c]\n",
    "                        Z[i, h, w, c] = self.convolution_single_step(a_slice_prev, weights, biases)\n",
    "\n",
    "        # Save information in \"cache\" for the backprop\n",
    "        cache = (A_prev, W, b, hparameters)\n",
    "\n",
    "        return Z, cache\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.62434536 -0.61175641]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 2)\n",
    "s, _ = x.shape\n",
    "print(x[0])\n",
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "temp = np.floor(3.7)\n",
    "print(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  3 40  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2], [3, 40]]\n",
    "a = np.pad(a, pad_width=(1, 1)  mode='constant', constant_values=(0, 0))\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.91797604 -0.85183742]\n",
      "  [ 0.09075857  1.16220247]]\n",
      "\n",
      " [[ 0.32178247  1.34131783]\n",
      "  [ 0.14167518  1.73738467]]\n",
      "\n",
      " [[ 0.37576878  1.47825568]\n",
      "  [-2.08016719 -0.45342757]]]\n",
      "[[-0.91797604]\n",
      " [-0.85183742]\n",
      " [ 0.09075857]\n",
      " [ 1.16220247]\n",
      " [ 0.32178247]\n",
      " [ 1.34131783]\n",
      " [ 0.14167518]\n",
      " [ 1.73738467]\n",
      " [ 0.37576878]\n",
      " [ 1.47825568]\n",
      " [-2.08016719]\n",
      " [-0.45342757]]\n",
      "(12, 1)\n"
     ]
    }
   ],
   "source": [
    "def flatten_matrix(input):\n",
    "    return input.flatten().reshape(-1, 1)\n",
    "\n",
    "x = np.random.randn(3, 2, 2)\n",
    "y = flatten_matrix(x)\n",
    "print(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [5 1]]\n",
      "[[2 4]\n",
      " [5 3]]\n",
      "[[ 4 16]\n",
      " [25  3]]\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.random.randint(low = 1, high = 8, size=(2, 2))\n",
    "y = np.random.randint(low = 1, high = 8, size=(2, 2))\n",
    "print(x)\n",
    "print(y)\n",
    "print(np.multiply(x, y))\n",
    "print(np.sum(np.multiply(x, y)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 9 9]\n",
      " [9 1 8]\n",
      " [3 1 8]]\n",
      "[[8 1 3]\n",
      " [8 1 9]\n",
      " [9 9 8]]\n"
     ]
    }
   ],
   "source": [
    "kernel = np.random.randint(low=1, high=10, size=(3,3))\n",
    "print(kernel)\n",
    "kernel = np.flipud(np.fliplr(kernel))\n",
    "print(kernel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def file_read(file_path: str):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines = [line.split() for line in lines]\n",
    "    lines = [[int(x) if idx != 0 else x for idx, x in enumerate(line)] for line in lines]\n",
    "\n",
    "    '''\n",
    "    *********************************** INPUT FORMAT *********************************************\n",
    "    Conv: the number of output channels, filter dimension, stride, padding.\n",
    "    ReLU: Activation layer.\n",
    "    Pool: filter dimension, stride.\n",
    "    FC: output dimension.\n",
    "    Flattening layer: it will convert a (series of) convolutional filter maps to a column vector.\n",
    "    Softmax: it will convert final layer projections to normalized probabilities.\n",
    "    '''\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def pad_input_2d_mat(X, pad: int, padding_value: int = 0):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C) # m is omitted for now\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, pad_width=((pad, pad), (pad, pad)),\n",
    "                   mode='constant', constant_values=(padding_value, padding_value))\n",
    "    # X_pad = np.pad(X, ((0,0),(pad,pad),(pad,pad),(0,0)))\n",
    "\n",
    "    return X_pad\n",
    "\n",
    "\n",
    "def cross_correlation2d(input_mat, filter_mat, stride, padding, result_shape):\n",
    "    filter_dimension, _ = filter_mat.shape\n",
    "    _, result_height, result_width = result_shape\n",
    "\n",
    "    result = np.zeros((result_height, result_width), dtype=float)\n",
    "\n",
    "    if padding != 0:\n",
    "        pad_input = pad_input_2d_mat(input_mat, pad=padding)\n",
    "    else:\n",
    "        pad_input = input_mat\n",
    "\n",
    "    for h in range(result_height):  # loop over vertical axis of the output volume\n",
    "        # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "        vertical_start = stride * h\n",
    "        vertical_end = vertical_start + filter_dimension\n",
    "\n",
    "        for w in range(result_width):  # loop over horizontal axis of the output volume\n",
    "            # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "            horizontal_start = stride * w\n",
    "            horizontal_end = horizontal_start + filter_dimension\n",
    "\n",
    "            input_slice = pad_input[vertical_start:vertical_end, horizontal_start:horizontal_end]\n",
    "            result[h, w] = np.sum(np.multiply(input_slice, filter_mat))\n",
    "            # print(result[h, w])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def convolution2d(input_mat, filter_mat, stride, padding, result_shape):\n",
    "    # Flip the filter\n",
    "    rotated_filter = np.flipud(np.fliplr(filter_mat))\n",
    "\n",
    "    return cross_correlation2d(input_mat, rotated_filter, stride, padding, result_shape)\n",
    "\n",
    "\n",
    "def flatten_matrix(input_data):\n",
    "    return input_data.flatten().reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n",
      "  [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]\n",
      "  [ 0.3190391  -0.24937038  1.46210794 -2.06014071]\n",
      "  [-0.3224172  -0.38405435  1.13376944 -1.09989127]]]\n",
      "[[1.62434536]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.86540763]\n",
      " [0.        ]\n",
      " [1.74481176]\n",
      " [0.        ]\n",
      " [0.3190391 ]\n",
      " [0.        ]\n",
      " [1.46210794]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [1.13376944]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(1, 4, 4)\n",
    "print(x)\n",
    "x = x.flatten().reshape(-1, 1)\n",
    "x = np.maximum(0, x)\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.14117648 0.7647059  0.27058825 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.6392157  1.         0.6392157  0.02352941 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.83137256 0.99607843 0.8627451  0.1882353  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.83137256 0.9254902  0.49803922 0.07450981 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.83137256 0.827451   0.44313726 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.43529412 0.99607843 0.5372549  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.5176471  0.99607843 0.43529412 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.49019608 0.99607843 0.40392157 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.46666667 0.99607843 0.05490196 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.16078432\n",
      "   0.8156863  0.99607843 0.27058825 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.07450981 0.98039216 0.43529412 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.69803923 0.43529412 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.69803923 0.78039217 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.6862745  0.8862745  0.07843138 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.23137255 0.9843137  0.4392157  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.8862745  0.7647059  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.5647059  0.9647059  0.12941177 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.27450982 0.9490196  0.56078434 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.47843137 0.8509804  0.13725491\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.12941177 0.73333335 0.68235296\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_test[100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4\n",
      "[[[[ 0.20937219 -1.14722491 -1.73588297  0.06441288]\n",
      "   [-0.89364831 -1.45206864 -0.05235459 -1.45838667]\n",
      "   [ 1.09016717  2.43782301 -1.20939821  1.15258488]]\n",
      "\n",
      "  [[ 0.51603118 -0.08547172 -0.77304463 -1.82140248]\n",
      "   [-0.02846136 -0.22934414 -0.16255177 -2.11475618]\n",
      "   [-0.91421322  0.25813814  0.6433904   0.75538222]]]]\n",
      "[[[[0.63817856 0.69648054 0.11644818 0.776604  ]\n",
      "   [0.40350945 0.49764749 0.53628667 0.70427408]\n",
      "   [0.37755531 0.94384    0.45245379 0.99497421]]\n",
      "\n",
      "  [[0.4470628  0.43569936 0.9552655  0.49652635]\n",
      "   [0.44797672 0.33654175 0.03783999 0.05457376]\n",
      "   [0.89767449 0.98379996 0.5414832  0.65960349]]]]\n"
     ]
    }
   ],
   "source": [
    "f_size = (1,2,3,4)\n",
    "print(*f_size)\n",
    "ara = np.random.randn(*f_size)\n",
    "ara1 = np.random.sample(f_size)\n",
    "\n",
    "print(ara)\n",
    "print(ara1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}